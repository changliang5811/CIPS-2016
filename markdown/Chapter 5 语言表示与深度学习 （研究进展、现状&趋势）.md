# Chapter 5 语言表示与深度学习
（研究进展、现状&趋势）

## 任务定义、目标&研究意义

### 对人类语言的一种描述或约定

- 在认知科学里，语言表示是语言在人脑中的表现形式，关系到人类如何理解和产生 语言
- 在人工智能里，语言表示主要指用于语言的形式化或数学的描述，以便在计算机中表示语言，并能让计算机程序自动处理。

	- 设计一种计算机内部的数据结构来表示语言，以及语言和此数据结构之间的相互转换机制

### 挑战：人类语言需结合一 定的上下文和知识才能理解

### 语言表示是自然语言处理以及语义计算的基础。

语言具有一定的层次结构，具体表现为 词、短语、句子、段落以及篇章等不同的语言粒度。为了让计算机可以理解语言，需要将不 同粒度的语言都转换成计算机可以处理的数据结构。


### 表示方法

- 语言表示模型划分

	- 

		- CBOW：根据中心词的上下文预测中心词的概率
		- Skip-Gram

			- 用中心词最大化预测输出层为上下文词汇的概率

- 早期：符号化的离散表示

	- 词：One-Hot向量
	- 句/篇：词袋模型、TF-IDF 模型、N 元模型等方法进行转换
	- 缺点

		- 词与词之间没有距离的概念

			- 需要引入人工知识库，比如同义词词 典、上下位词典等，才能有效地进行后续的语义计算
			- 改进方法

				- 基于聚类的词表示

		- 无法解决”多词一义“问题

- 连续表示

	- 为了解决离散表示所无法解决的““一词多义”和“一义多词”问题
	- 将语言单位表示为连续语义空间中的一个点，这样的表示方法称之为连续表示
	- 基于连续表示，词与词之间 就可以通过欧式距离或余弦距离等方式来计算相似度
	- 常用方法

		- 分布式表示

			- 基于 Harris 的分布式假设，即如果两个词的上下文相似，那么这两个词也是相似的。

		- 分散式表示（主流方法）

			- 将语言的潜在语法或语义特征分散式地存储在一组神经元中，可以用稠密、低维、连续的向量来表示，也叫嵌入（Embeddings）。
			- 一个好的词嵌入模型应该是：对于 相似的词，它们对应的词嵌入也相近
			- 根据所表示文本的颗粒度的不同，可以分为词、句子、篇章的表示

				- 词表示

					- Word embeddings
					- 词嵌 入的质量非常依赖于上下文窗口大小的选择
					- 有研究者关注如何利用已有的知识库来改进词嵌入模型，结合知识图谱 和未标注语料在同一语义空间中来联合学习知识和词的向量表示，这样可以更有效地实现词 的嵌入

				- 句子表示

					- 很多任务的输入是变长的文本序列，需要将变长的文本序列表示成固定长度的向量
					- 句子编码主要研究如何有效地从词嵌入通过不同 方式的组合得到句子表示

						- 神经词袋模型

							- 简单对文本序列中每个词嵌入进行平均，作为整个序列的表示
							- 缺点

								- 丢失了词序信息
								- 对于长文本，神经词袋模型比较有效。但是对于短文本， 神经词袋模型很难捕获语义组合信息

						- 递归神经网络（Recursive Neural Network）

							- 按照一个给定的外部拓扑 结构（比如成分句法树），不断递归得到整个序列的表示
							- 缺点：需要 给定一个拓扑结构来确定词和词之间的依赖关系，因此限制其使用范围

						- 循环神经网络（Recurrent Neural Network）

							- 将文本序列看作时间序列，不 断更新，最后得到整个序列的表示

						- 卷积神经网络（Convolutional Neural Network）

							- 通过多个卷积层和子采样 层，最终得到一个固定长度的向量。

						- 改进

							- 综合这些方法的优点，结合具体的任务，已 经 提出了一些更复杂的组合模型， 例如 双向循环神经 网络 （ Bi-directional Recurrent Neural Network）、长短时记忆模型（Long-Short Term Memory）等。
							- 比如近几年大热的Attention机制、Transformer以及Bert模型

				- 篇章表示

					- 思想：层次化的方法，先得到句子编码，然后以句子编码为输入，进一步得到篇章的表示
					- 层次化CNN

						- 用卷积神经网络对每个句子进行建模，然后以 句子为单位再进行一次卷积和池化操作，得到篇章表示

					- 层次化RNN

						- 用循环神经网络对每个句子进行建模，然后再用一个循环神经网络建模以句子为单位的序列，得到篇章表示

					- 混合模型

						- 先用循环神经网络对每个句子进行建模，然后以句子为单位再进行 一次卷积和池化操作，得到篇章表示

					- 循环神经网络因为非常适合处理文本 序列，因此被广泛应用在很多自然语言处理任务上。

		- 区别：分散式表示是指一种语义分散存储的表示形式，而分布式 表示是通过分布式假设获得的表示
		- 联系：两者并不对立，比如 Skip-Gram、CBOW 和 glove 等模型得到词向量，即是分散式表示，又是分布式表示。

## 关键科学问题和研究内容

### 研究内容

- 如何针对不同的语言单位，设计表示语言的数据结构以及和语言的转换机制。即如何将语言转换成计算机内部的数据结构（理解）以及由计算机内部表示转换成语言（生成）。

### 关键科学问题

- 语言表示的认知机理

	- 一个高效的语言表示模型需要借鉴人类的认知机理
	- 人们对语言的理解需要大量的背景知识
	- 语言表示和知识表示应该是相辅相成的
	- 关键问题：如何构建语言表示和知识表示的联系，从人工知识库或大规模未 标记语料来自动学习语言的表示

- 跨语种的统一语言表示

	- 不同语种的语言表示也具有一定的相似性，即可以用同一种表示方式来刻画 不同语言
	- 关键问题：如何为不同语种构建一种统一的语言表示模型，利用不同语言之间的共性， 从而提高各个语言的表示能力

- 不同粒度单位的语言表示 

	- 字、词、句子、篇章等不同粒度或层次的语 下文进行理解，如“一词多义”问题
	- 关键问题：结合语言本身的层次结构以及不同粒度文本之间的制约关 系，构建一个多粒度文本的联合语义表示模型

- 基于少量观察样本的新词、低频词表示学习

	- 目前，词的表示是通过大量的语料库学习得到的
	- 语言中低频词往 含有价值的信息，丢弃这些词也往往降低了语言表示的能力
	- 人们学习新词和低频词的方 式并不是通过大量语料进行学习的，而是通过字典或少量观察样本进行学习
	- 关键问题：对于新 词或低频词，需要研究如何通过少量观察样本来学习新词和低频词的表示

## 技术方法和研究现状

### 语言表示方法

- 按不同粒度进行划分，语言 具有一定的层次结构，语言表示可以分为字、词、句子、篇章等不同粒度的表示
- 按表示形式进行划分，可以分为离散表示和连续表示两类
（具体见任务定义、目标&研究意义——表示方法）

	- 离散表示是将语言看成离散 的符号
	- 连续表示将语言表示为连续空间中的一个点，包括分布式表示和分散式表示

## 技术展望&发展趋势

### 目前，基于深度学习的方法在自然语言处理中取得了很大的进展，因此，分散式表示也 成为语言表示中最热门的方法

### 关注问题

- 语言中出现所有符号是否都需要使用统一的表示模型？比如，无意义 数字等
-  新词以及低频词的表示学习方法。目前的表示学习方法很难对这些词进行很好的建 模，而这些词都是极具信息量的，不能简单忽略
- 篇章的语言表示。目前对篇章级别的文本进行建模方法比较简单，不足以表示篇章 中的复杂语义
- 语言表示的基础数据结构。除了目前的基于向量的数据结构 结构，比如矩阵、队列、栈等

### 随着深度学习、无监督学习、以及增强学习等技术的快速发展以及大量文本数据的涌现， 语言表示作为自然语言处理中最基础的问题将会得到相当程度的解决，从而为下游的各种自 然语言处理任务，诸如机器翻译、自动文摘、文本分类、自动问答等，提供有效的表示基础。

